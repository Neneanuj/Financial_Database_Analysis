# Findata - Financial Statement Database Project 
### **ğŸ“„ Project Summary**  
Demo Link: 

ğŸš€ **Extract, Standardize, and Store Data from PDFs and Webpages**  

## **ğŸ“Œ Overview**
The goal of Findata Inc. is to create a consolidated library of financial statements that will assist analysts in performing fundamental analysis on publicly traded US corporations.  

The system provides:  
âœ… **Snowflake** for data storage and querying
âœ… **SEC Financial Statement Data Sets** for raw financial data
âœ… **DBT** for data transformations
âœ… **Airflow** for data pipelines and automation
âœ… **FastAPI backend** for document processing  
âœ… **Streamlit web interface** for user-friendly interaction  
âœ… **Cloud storage (AWS S3)** for processed files  

---

## **ğŸ”‘ Features**
âœ… **Automated Data Ingestion:** Scrapes SEC datasets and loads them into Snowflake. 
âœ… **Data Transformation:** Merges, cleans, and transforms data into JSON and normalized formats.
âœ… **Pipeline Orchestration:** Airflow manages all ETL tasks.  
âœ… **Interactive Dashboard:** Streamlit app provides an easy-to-use interface for data exploration. 
âœ… **Store data in AWS S3**  
âœ… **Provide an API with FastAPI:** FastAPI backend for programmatic access to financial data.  
âœ… **User-friendly Streamlit Web App**   

---

## **âœ” Technology Stack**

| **Category**       | **Tools Used** |
|------------------|--------------|
| **Programming Language** | Python 3.8+ |
| **Snowflake** | Cloud data platform for storage and queryinge. |
| **Apache Airflow** | Workflow automation and scheduling. |
| **DBT (Data Build Tool)** | Data transformations and modeling. |
| **Backend API** | FastAPI |
| **Frontend UI** | Streamlit |
| **Cloud Storage** | AWS S3 |
| **Deployment** | Render, Streamlit Cloud |

---

## **ğŸ› ï¸ Diagrams**

![image](./docs/data_extraction_architecture.png)


---

## **ğŸ“‚ Project Structure**
```plaintext
findata/
â”œâ”€â”€ data_ingestion/               # Task 1: Data Ingestion
â”‚   â”œâ”€â”€ sec_scraper.py            # SEC link scraping script
â”‚   â””â”€â”€ sample_dataset/           # Sample dataset files (for local testing)
â”‚
â”œâ”€â”€ storage/                      # Task 2: Storage Design
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”œâ”€â”€ raw_schema.sql        # Raw schema DDL
â”‚   â”‚   â”œâ”€â”€ json_schema.sql       # JSON schema DDL
â”‚   â”‚   â””â”€â”€ normalized_schema.sql # Normalized schema DDL
â”‚   â””â”€â”€ snowflake_loader.py       # Unified data loader
â”‚
â”œâ”€â”€ dbt_transform/                # Task 3: DBT Transformation
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/              # Three staging models
â”‚   â”‚   â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”‚   â”œâ”€â”€ json/ 
â”‚   â”‚   â”‚   â””â”€â”€ normalized/
â”‚   â”‚   â””â”€â”€ marts/                # Unified business models
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”œâ”€â”€ quality_checks.sql    # Data quality checks
â”‚   â”‚   â””â”€â”€ schema_validation.yml # Schema validation
â”‚   â””â”€â”€ docs/strategy.md          # DBT strategy documentation
â”‚
â”œâ”€â”€ airflow/                      # Task 4: Pipelines
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ raw_pipeline.py       # Raw data processing DAG
â”‚   â”‚   â”œâ”€â”€ json_pipeline.py      # JSON transformation DAG
â”‚   â”‚   â””â”€â”€ normalized_pipeline.py# Normalized data DAG
â”‚   â””â”€â”€ config/
â”‚       â”œâ”€â”€ base_config.yaml      # General configuration
â”‚       â””â”€â”€ q4202-teamX.yaml      # Quarter-specific configuration
â”‚
â”œâ”€â”€ tests/                        # Task 5: Testing
â”‚   â”œâ”€â”€ integration/              # Integration tests
â”‚   â”œâ”€â”€ data_validation.py        # Data validation logic
â”‚   â””â”€â”€ test_report.md            # Test documentation template
â”‚
â”œâ”€â”€ app/                          # Task 6: Application Layer
â”‚   â”œâ”€â”€ frontend/                 # Streamlit
â”‚   â”‚   â”œâ”€â”€ app.py
â”‚   â”‚   â””â”€â”€ queries/              # Example queries for all three storage options
â”‚   â””â”€â”€ backend/                  # FastAPI
â”‚       â”œâ”€â”€ api.py
â”‚       â””â”€â”€ snowflake_client.py   # Unified Snowflake client
â”‚
â”œâ”€â”€ docs/                         # Documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md           # Architecture design
â”‚   â”œâ”€â”€ TESTING.md                # Testing documentation
â”‚   â””â”€â”€ AI_USAGE.md               # AI usage disclosure
â”‚
â””â”€â”€ config/                       # Environment Configuration
    â”œâ”€â”€ snowflake_creds.json
    â””â”€â”€ s3_conn.yml

```

---

## **ğŸš€ Installation & Setup**
1ï¸âƒ£ Prerequisites
Ensure you have:

Python 3.8 or higher
pip (Python package manager)
AWS credentials (if storing files in AWS S3)

2ï¸âƒ£ Clone the Repository
```
git clone https://github.com/yourusername/findata-financial-db.git
cd findata-financial-db
```

3ï¸âƒ£ Install Dependencies
```
pip install -r requirements.txt
```

4ï¸âƒ£ Configure Environment
* Snowflake Credentials: Add your credentials to 
```
config/snowflake_creds.json.
```
* S3 Connection: Add your S3 details to 
```
config/s3_conn.yml.
```

5ï¸âƒ£ Run Data Ingestion
```
python data_ingestion/sec_scraper.py
python data_ingestion/sec_csv_reader.py
```

6ï¸âƒ£ Load Data into Snowflake
```
python storage/snowflake_loader.py
```

7ï¸âƒ£ Run Airflow Pipelines

* Start Docker and run Airflow:
```
docker-compose up -d
```
* Access the Airflow web interface at http://localhost:8080.
* Trigger the DAGs for data processing.

8ï¸âƒ£ Deploy and Access Applications
* Streamlit App: Run the frontend:
```
cd app/frontend
streamlit run app.py
```
* FastAPI Backend: Run the backend:
```
cd app/backend
uvicorn api:app --reload
```

9ï¸âƒ£ Testing
Run tests:
```
pytest tests/
```

---

## **ğŸ“Œ Expected Outcomes**

* Merged SEC Data: CSV files from SEC (sub.txt, num.txt) are merged into clean, structured data.
* Clean Data: Data is cleaned, transformed, and validated using Python scripts and DVT tools.
* JSON Storage: Transformed data stored in Snowflake in JSON format.
* Automated Pipelines: Airflow pipelines handle ingestion, validation, and storage automatically.
* Accessible Applications: Streamlit app and FastAPI backend for querying and visualizing financial data.

---

## **ğŸ“Œ AI Use Disclosure**
This project uses:

ğŸ“„ See AiUseDisclosure.md for details.

---

## **ğŸ‘¨â€ğŸ’» Authors**
* Sicheng Bao (@Jellysillyfish13)
* Yung Rou Ko (@KoYungRou)
* Anuj Rajendraprasad Nene (@Neneanuj)

---

## **ğŸ“ Contact**
For questions, reach out via Big Data Course or open an issue on GitHub.
